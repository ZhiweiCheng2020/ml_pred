# SAINT (Self-Attention and Intersample Attention Transformer) configuration

model_name: "saint"
model_type: "neural"

# Whether to apply feature selection for this model
apply_feature_selection: false  # SAINT uses self-attention for feature importance

# Model architecture parameters
architecture:
  dim: 64  # Dimension of the model (d_model) - increased for capacity
  depth: 4  # Number of transformer layers - reduced for efficiency
  heads: 8  # Number of attention heads
  dim_head: 8  # Dimension of each attention head (dim / heads)
  attn_dropout: 0.1  # Dropout for attention layers
  ff_dropout: 0.1  # Dropout for feed-forward layers
  use_intersample_attention: false  # Disable for speed on large datasets
  d_ff_multiplier: 4  # Feed-forward dimension multiplier (d_ff = dim * d_ff_multiplier)
  output_dim: 3  # MINIMAL CHANGE: Set to 3 for multi-output

# Training parameters
training:
  batch_size: 256  # Larger batches for efficiency
  epochs: 150  # Fewer epochs to reduce training time
  learning_rate: 0.001
  optimizer: "adamw"  # Options: "adam", "adamw", "sgd"
  weight_decay: 0.01
  warmup_epochs: 5  # Reduced warmup
  gradient_accumulation_steps: 1
  mixed_precision: true  # Enable for faster training with GPUs

  # Learning rate scheduler
  scheduler:
    enabled: true
    type: "cosine"  # Options: "cosine", "reduce_on_plateau"
    min_lr: 0.00001
    patience: 15  # For reduce_on_plateau
    factor: 0.5   # For reduce_on_plateau

  # Early stopping
  early_stopping:
    enabled: true
    patience: 30
    min_delta: 0.00001
    monitor: "val_loss"  # Metric to monitor

# Data augmentation (specific to SAINT)
aug