# Sparse Neural Network with L1 Regularization configuration

model_name: "sparse_nn"
model_type: "neural"

# Whether to apply feature selection for this model
apply_feature_selection: true  # Reduce input dimension

# Model architecture
architecture:
  input_dim: null  # Will be set dynamically based on features
  hidden_layers:
    - units: 512  # Larger for more capacity
      activation: "relu"
      dropout: 0.3
      l1_regularization: 0.001  # Lighter L1
      batch_norm: true
    - units: 256
      activation: "relu"
      dropout: 0.2
      l1_regularization: 0.0005
      batch_norm: true
    - units: 128
      activation: "relu"
      dropout: 0.1
      l1_regularization: 0.0001
      batch_norm: true
    - units: 64
      activation: "relu"
      dropout: 0.05
      l1_regularization: 0
      batch_norm: true
  output_dim: 3
  output_activation: "linear"

# Training parameters
training:
  batch_size: 512  # Much larger for 200k samples
  epochs: 300
  learning_rate: 0.001
  optimizer: "adam"  # Options: "adam", "sgd", "rmsprop"
  loss: "mse"
  early_stopping:
    enabled: true
    patience: 30
    min_delta: 0.00001
    restore_best_weights: true
  reduce_lr:
    enabled: true
    patience: 15
    factor: 0.5
    min_lr: 0.00001
  validation_split: 0.15
  shuffle: true

# Regularization
regularization:
  l1: 0.001  # Overall L1 regularization for sparsity
  l2: 0.0001  # Light L2 regularization
  batch_norm: true
  dropout_rate: 0.2
  gradient_clip_norm: 1.0  # Gradient clipping for stability

# Hyperparameter tuning
hyperparameter_tuning:
  enabled: true
  method: "optuna"
  n_trials: 20  # Expensive to train
  cv_folds: 2  # Fewer folds for neural nets
  param_space:
    learning_rate: [0.0001, 0.01]
    batch_size: [256, 1024]
    l1_regularization: [0.0001, 0.01]
    l2_regularization: [0.00001, 0.001]
    dropout_rate: [0.1, 0.4]
    n_layers: [3, 5]
    n_units: [64, 512]